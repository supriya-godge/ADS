{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatically log in to Freddie Mac website and download all the sample files from 2005 onwards based on request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the required modules\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as rq\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import lxml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logging in to the website and downloading the sample files from 2005 onwards \n",
    "def downloadSampleFiles():\n",
    "    LOGIN_URL = \"https://freddiemac.embs.com/FLoan/secure/auth.php\"\n",
    "    URL = \"https://freddiemac.embs.com/FLoan/Data/download.php\"\n",
    "    with requests.session() as c:\n",
    "    \n",
    "        payload = {'username': 'prashantvksingh@gmail.com', 'password': 'Ksov]r<h',\\\n",
    "                  'action': 'acceptTandC', 'acceptSubmit': 'Continue', 'accept':'Yes'}\n",
    "    \n",
    "        login_response = c.post(LOGIN_URL, data = payload)\n",
    "        print('Logged in to the website!!', '\\n')\n",
    "    \n",
    "        download_response = c.post(URL, data=payload)\n",
    "\n",
    "        list_of_links = find_files(download_response)\n",
    "        print('Collected the required file links!!', '\\n')\n",
    "\n",
    "        year_list = list(range(2005,2017))\n",
    "        files_required = []\n",
    "    \n",
    "    download_path = os.path.expanduser('~') + '\\\\Midterm\\\\Data\\\\SampleFiles\\\\'\n",
    "    if not os.path.exists(download_path):\n",
    "        print('Creating required directories!!', '\\n')\n",
    "        os.makedirs(download_path)\n",
    "        print('Starting sample files download!!', '\\n')\n",
    "        count = 0\n",
    "        for link in list_of_links:\n",
    "            for year in year_list:\n",
    "                if ('sample_' + str(year)) in link:\n",
    "                    count = count + 1\n",
    "                if count > 0:        \n",
    "                    files_required.append([link, 'sample_' + str(year)])\n",
    "                count = 0      \n",
    "        \n",
    "            for file, filename in files_required:\n",
    "                samplefile_response = c.get(file)\n",
    "                samplefile_content = ZipFile(BytesIO(samplefile_response.content)) \n",
    "                samplefile_content.extractall(download_path + filename)\n",
    "                \n",
    "        print('Sample files downloaded in the path: ' + download_path, '\\n')\n",
    "    else:\n",
    "        print('Sample files are already present!!', '\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Created a function that retrieves the link that needs to be downloaded\n",
    "def find_files(response):\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    download_url = 'https://freddiemac.embs.com/FLoan/Data/'\n",
    "    hrefs = []\n",
    "    \n",
    "    for a in soup.find_all('a'):\n",
    "        hrefs.append(download_url + a['href'])\n",
    "        \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orig_fillNA(orig_df):\n",
    "    orig_df['cred_scr'] = orig_df['cred_scr'].fillna(0)\n",
    "    orig_df['fst_hmebyr_flg']=orig_df['fst_hmebyr_flg'].fillna('Unknown')\n",
    "    orig_df['metro_stat_area']=orig_df['metro_stat_area'].fillna(0)\n",
    "    orig_df['mort_insur_pctg']=orig_df['mort_insur_pctg'].fillna(0)\n",
    "    orig_df['nbr_units']=orig_df['nbr_units'].fillna(0)\n",
    "    orig_df['occu_status']=orig_df['occu_status'].fillna('Unknown')\n",
    "    orig_df['orig_cmbnd_ln_to_value']=orig_df['orig_cmbnd_ln_to_value'].fillna(0)\n",
    "    orig_df['orig_dbt_to_incm']=orig_df['orig_dbt_to_incm'].fillna(0)\n",
    "    orig_df['orig_ln_to_value']=orig_df['orig_ln_to_value'].fillna(0)\n",
    "    orig_df['chnl']=orig_df['chnl'].fillna('Unknown')\n",
    "    orig_df['pre_pnl_mort_flg']=orig_df['pre_pnl_mort_flg'].fillna('Unknown')\n",
    "    orig_df['proptype']=orig_df['proptype'].fillna('Unknown')\n",
    "    orig_df['zipcode']=orig_df['zipcode'].fillna(0)\n",
    "    orig_df['ln_purps']=orig_df['ln_purps'].fillna('Unknown')\n",
    "    orig_df['nbr_brwrs']=orig_df['nbr_brwrs'].fillna(0)\n",
    "    orig_df['spr_confrm_flg']=orig_df['spr_confrm_flg'].fillna('N')\n",
    "    \n",
    "    return orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combineOrigFiles():\n",
    "    print('Starting creation of combined Origination File!!', '\\n')\n",
    "    filedirectory_path = os.path.expanduser('~') + '\\\\Midterm\\\\Data\\\\SampleFiles\\\\'\n",
    "    combinedfile_name = filedirectory_path + 'OriginationCombined.csv'\n",
    "    firstRun = 1\n",
    "    \n",
    "    for path, subdirs, files in os.walk(filedirectory_path):\n",
    "        for file in files:\n",
    "            if '_orig_' in file:\n",
    "                actual_file = os.path.join(path, file)\n",
    "                \n",
    "                with open(actual_file, 'r', encoding='utf-8') as f:\n",
    "                    orig_df = pd.read_csv(f, sep='|', names=['cred_scr', 'fst_paymnt_dte', 'fst_hmebyr_flg', 'maturty_dte', \\\n",
    "                                                             'metro_stat_area', 'mort_insur_pctg', 'nbr_units', 'occu_status',\\\n",
    "                                                             'orig_cmbnd_ln_to_value', 'orig_dbt_to_incm', 'orig_upb', \\\n",
    "                                                             'orig_ln_to_value', 'orig_intrst_rate', 'chnl', 'pre_pnl_mort_flg',\\\n",
    "                                                             'prodtype', 'propstate', 'proptype', 'zipcode', 'ln_sq_nbr', \\\n",
    "                                                             'ln_purps', 'orig_ln_trm', 'nbr_brwrs', 'slr_name', 'srvcr_name',\\\n",
    "                                                             'spr_confrm_flg'], \\\n",
    "                                          skipinitialspace=True, low_memory=False)\n",
    "                    orig_df = orig_fillNA(orig_df)\n",
    "                    orig_df[['cred_scr','metro_stat_area','mort_insur_pctg','nbr_units','orig_cmbnd_ln_to_value',\\\n",
    "                             'orig_dbt_to_incm','orig_upb','orig_ln_to_value','zipcode','orig_ln_trm', 'nbr_units']] = \\\n",
    "                            orig_df[['cred_scr','metro_stat_area','mort_insur_pctg','nbr_units','orig_cmbnd_ln_to_value',\\\n",
    "                                'orig_dbt_to_incm','orig_upb','orig_ln_to_value','zipcode','orig_ln_trm',\\\n",
    "                                'nbr_units']].astype('int64')\n",
    "                    orig_df[['spr_confrm_flg','srvcr_name']] = orig_df[['spr_confrm_flg','srvcr_name']].astype('str')\n",
    "                    \n",
    "                    orig_df['Year_Orig'] = ['20'+x for x in (orig_df['ln_sq_nbr'].apply(lambda x: x[2:4]))]\n",
    "                    \n",
    "                    if firstRun == 1:\n",
    "                        orig_df.to_csv(combinedfile_name, mode='a', header=True,index=False)\n",
    "                        firstRun = 0\n",
    "                    else:\n",
    "                        orig_df.to_csv(combinedfile_name, mode='a', header=False,index=False)\n",
    "    print('Combined Origination file created successfully!!', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def performance_fillNA(perf_df):\n",
    "    perf_df['curr_ln_delin_status'] = perf_df['curr_ln_delin_status'].fillna(0)\n",
    "    perf_df['repurch_flag']=perf_df['repurch_flag'].fillna('Unknown')\n",
    "    perf_df['mod_flag']=perf_df['mod_flag'].fillna('N')\n",
    "    perf_df['zero_bal_cd']=perf_df['zero_bal_cd'].fillna(00)\n",
    "    perf_df['zero_bal_eff_dt']=perf_df['zero_bal_eff_dt'].fillna('999901')\n",
    "    perf_df['current_dupb']=perf_df['current_dupb'].fillna(0)\n",
    "    perf_df['lst_pd_inst_duedt']=perf_df['lst_pd_inst_duedt'].fillna('999901')\n",
    "    perf_df['mi_recoveries']=perf_df['mi_recoveries'].fillna(0)\n",
    "    perf_df['net_sale_proceeds']=perf_df['net_sale_proceeds'].fillna('U')\n",
    "    perf_df['non_mi_recoveries']=perf_df['non_mi_recoveries'].fillna(0)\n",
    "    perf_df['expenses']=perf_df['expenses'].fillna(0)\n",
    "    perf_df['legal_costs']=perf_df['legal_costs'].fillna(0)\n",
    "    perf_df['maint_pres_costs']=perf_df['maint_pres_costs'].fillna(0)\n",
    "    perf_df['taxes_and_insur']=perf_df['taxes_and_insur'].fillna(0)\n",
    "    perf_df['misc_expenses']=perf_df['misc_expenses'].fillna(0)\n",
    "    perf_df['actual_loss_calc']=perf_df['actual_loss_calc'].fillna(0)\n",
    "    perf_df['mod_cost']=perf_df['mod_cost'].fillna(0)\n",
    "    \n",
    "    return perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax(perf_df):\n",
    "    new1_df = perf_df.groupby(['ln_sq_nbr'])['current_aupb'].min().to_frame(name = 'min_current_aupb').reset_index()\n",
    "    new2_df = perf_df.groupby(['ln_sq_nbr'])['current_aupb'].max().to_frame(name = 'max_current_aupb').reset_index()\n",
    "    new3_df = perf_df.groupby(['ln_sq_nbr'])['curr_ln_delin_status'].min().to_frame(name = 'min_curr_ln_delin_status').reset_index()\n",
    "    new4_df = perf_df.groupby(['ln_sq_nbr'])['curr_ln_delin_status'].max().to_frame(name = 'max_curr_ln_delin_status').reset_index()\n",
    "    new5_df = perf_df.groupby(['ln_sq_nbr'])['zero_bal_cd'].min().to_frame(name = 'min_zero_bal_cd').reset_index()\n",
    "    new6_df = perf_df.groupby(['ln_sq_nbr'])['zero_bal_cd'].max().to_frame(name = 'max_zero_bal_cd').reset_index()\n",
    "    new7_df = perf_df.groupby(['ln_sq_nbr'])['mi_recoveries'].min().to_frame(name = 'min_mi_recoveries').reset_index()\n",
    "    new8_df = perf_df.groupby(['ln_sq_nbr'])['mi_recoveries'].max().to_frame(name = 'max_mi_recoveries').reset_index()\n",
    "    new11_df = perf_df.groupby(['ln_sq_nbr'])['non_mi_recoveries'].min().to_frame(name = 'min_non_mi_recoveries').reset_index()\n",
    "    new12_df = perf_df.groupby(['ln_sq_nbr'])['non_mi_recoveries'].max().to_frame(name = 'max_non_mi_recoveries').reset_index()\n",
    "    new13_df = perf_df.groupby(['ln_sq_nbr'])['expenses'].min().to_frame(name = 'min_expenses').reset_index()\n",
    "    new14_df = perf_df.groupby(['ln_sq_nbr'])['expenses'].max().to_frame(name = 'max_expenses').reset_index()\n",
    "    new15_df = perf_df.groupby(['ln_sq_nbr'])['legal_costs'].min().to_frame(name = 'min_legal_costs').reset_index()\n",
    "    new16_df = perf_df.groupby(['ln_sq_nbr'])['legal_costs'].max().to_frame(name = 'max_legal_costs').reset_index()\n",
    "    new17_df = perf_df.groupby(['ln_sq_nbr'])['maint_pres_costs'].min().to_frame(name = 'min_maint_pres_costs').reset_index()\n",
    "    new18_df = perf_df.groupby(['ln_sq_nbr'])['maint_pres_costs'].max().to_frame(name = 'max_maint_pres_costs').reset_index()\n",
    "    new19_df = perf_df.groupby(['ln_sq_nbr'])['taxes_and_insur'].min().to_frame(name = 'min_taxes_and_insur').reset_index()\n",
    "    new20_df = perf_df.groupby(['ln_sq_nbr'])['taxes_and_insur'].max().to_frame(name = 'max_taxes_and_insur').reset_index()\n",
    "    new21_df = perf_df.groupby(['ln_sq_nbr'])['misc_expenses'].min().to_frame(name = 'min_misc_expenses').reset_index()\n",
    "    new22_df = perf_df.groupby(['ln_sq_nbr'])['misc_expenses'].max().to_frame(name = 'max_misc_expenses').reset_index()\n",
    "    new23_df = perf_df.groupby(['ln_sq_nbr'])['actual_loss_calc'].min().to_frame(name = 'min_actual_loss_calc').reset_index()\n",
    "    new24_df = perf_df.groupby(['ln_sq_nbr'])['actual_loss_calc'].max().to_frame(name = 'max_actual_loss_calc').reset_index()\n",
    "    new25_df = perf_df.groupby(['ln_sq_nbr'])['mod_cost'].min().to_frame(name = 'min_mod_cost').reset_index()\n",
    "    new26_df = perf_df.groupby(['ln_sq_nbr'])['mod_cost'].max().to_frame(name = 'max_mod_cost').reset_index()\n",
    "    \n",
    "    final_df = new1_df.merge(new2_df,on='ln_sq_nbr').merge(new3_df,on='ln_sq_nbr').merge(new4_df,on='ln_sq_nbr').\\\n",
    "    merge(new5_df,on='ln_sq_nbr').merge(new6_df,on='ln_sq_nbr').merge(new7_df,on='ln_sq_nbr').merge(new8_df,on='ln_sq_nbr').\\\n",
    "    merge(new11_df,on='ln_sq_nbr').merge(new12_df,on='ln_sq_nbr').merge(new13_df,on='ln_sq_nbr').merge(new14_df,on='ln_sq_nbr').\\\n",
    "    merge(new15_df,on='ln_sq_nbr').merge(new16_df,on='ln_sq_nbr').merge(new17_df,on='ln_sq_nbr').merge(new18_df,on='ln_sq_nbr').\\\n",
    "    merge(new19_df,on='ln_sq_nbr').merge(new20_df,on='ln_sq_nbr').merge(new21_df,on='ln_sq_nbr').merge(new22_df,on='ln_sq_nbr').\\\n",
    "    merge(new23_df,on='ln_sq_nbr').merge(new24_df,on='ln_sq_nbr').merge(new25_df,on='ln_sq_nbr').merge(new26_df,on='ln_sq_nbr')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combinePerfFiles():\n",
    "    print('Starting creation of combined Performance File!!', '\\n')\n",
    "    filedirectory_path = os.path.expanduser('~') + '\\\\Midterm\\\\Data\\\\SampleFiles\\\\'\n",
    "    combinedfile_name = filedirectory_path + 'PerformanceCombined.csv'\n",
    "    firstRun = 1\n",
    "    \n",
    "    for path, subdirs, files in os.walk(filedirectory_path):\n",
    "        for file in files:\n",
    "            if '_svcg_' in file:\n",
    "                actual_file = os.path.join(path, file)\n",
    "                \n",
    "                with open(actual_file, 'r', encoding='utf-8') as f:\n",
    "                    perf_df = pd.read_csv(f ,sep=\"|\", names=['ln_sq_nbr','mon_rpt_prd','current_aupb','curr_ln_delin_status',\\\n",
    "                                                             'loan_age','remng_mon_to_leg_matur', 'repurch_flag','mod_flag', \\\n",
    "                                                             'zero_bal_cd', 'zero_bal_eff_dt','current_int_rte','current_dupb',\\\n",
    "                                                             'lst_pd_inst_duedt','mi_recoveries', 'net_sale_proceeds',\\\n",
    "                                                             'non_mi_recoveries','expenses', 'legal_costs', 'maint_pres_costs',\\\n",
    "                                                             'taxes_and_insur','misc_expenses','actual_loss_calc', 'mod_cost'],\\\n",
    "                                          skipinitialspace=True, low_memory=False)\n",
    "                    \n",
    "                    perf_df['curr_ln_delin_status'] = [ 999 if x=='R' else x for x in \\\n",
    "                                                       (perf_df['curr_ln_delin_status'].apply(lambda x: x))]\n",
    "                    perf_df['curr_ln_delin_status'] = [ 0 if x=='XX' else x for x in \\\n",
    "                                                       (perf_df['curr_ln_delin_status'].apply(lambda x: x))]\n",
    "                    perf_df = performance_fillNA(perf_df)\n",
    "                    \n",
    "                    perf_df[['curr_ln_delin_status','loan_age','remng_mon_to_leg_matur','zero_bal_cd','current_dupb',\\\n",
    "                             'actual_loss_calc']] = perf_df[['curr_ln_delin_status','loan_age','remng_mon_to_leg_matur',\\\n",
    "                                                             'zero_bal_cd','current_dupb','actual_loss_calc']].astype('int64')\n",
    "\n",
    "                    perf_df[['mon_rpt_prd','zero_bal_eff_dt','lst_pd_inst_duedt']] = perf_df[['mon_rpt_prd','zero_bal_eff_dt',\\\n",
    "                                                                                              'lst_pd_inst_duedt']].astype('str')\n",
    "                    \n",
    "                    filtered_df = minmax(perf_df)\n",
    "                    \n",
    "                    filtered_df['Year_Perf'] = ['20'+x for x in (filtered_df['ln_sq_nbr'].apply(lambda x: x[2:4]))]\n",
    "                    \n",
    "                    if firstRun == 1:\n",
    "                        filtered_df.to_csv(combinedfile_name, mode='a', header=True,index=False)\n",
    "                        firstRun = 0\n",
    "                    else:\n",
    "                        filtered_df.to_csv(combinedfile_name, mode='a', header=False,index=False)\n",
    "    print('Combined Performance file created successfully!!', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    downloadSampleFiles()\n",
    "    combineOrigFiles()\n",
    "    combinePerfFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting creation of combined Origination File!! \n",
      "\n",
      "Combined Origination file created successfully!! \n",
      "\n",
      "Starting creation of combined Performance File!! \n",
      "\n",
      "Combined Performance file created successfully!! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "combineOrigFiles()\n",
    "combinePerfFiles()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
